# RL Safety Workshop (RLSW) @ RLC 2024

Workshop at the [Reinforcement Learning Conference 2024](https://rl-conference.cc/index.html).

August 9-12, 2024 \
Amherst, MA

## Workshop Summary

The RL Safety Workshop will focus on the study of safe reinforcement learning systems that are aligned with human values. The workshop will blend research from several branches of reinforcement learning, ranging from theoretical formalisms of alignment, to strategies for building safe RL agents, to auditing and monitoring of deployed systems. We hope these topics will be of interest to a large subset of the RLC community, as they are crucial for ensuring that increasingly powerful RL systems remain beneficial to humanity.

## Topics of Interest

The following is a more detailed list of potential topic areas:

- Models/formalisms of safety/alignment/ethics in RL
- RL generalization and robustness
- Using LLMs/foundation models for sequential decision making
- Multi-agent RL
- Human-in-the-loop/cooperative RL
- Human value learning and inverse RL
- Using RL for algorithmic recommendations
- RL interpretability/explainability
- Fairness and bias in RL
- Governance/policy/law for RL systems
- Auditing and deployment safeguards for RL systems

## Important Dates

- Paper submission deadline: TBD
- Paper acceptance notification: TBD

## Submission Details

TBD

## Organizing Committee

- Cameron Allen, UC Berkeley
- Serena Booth, US Senate
- Micah Carroll, UC Berkeley
- Davis Foote, UC Berkeley
- Dylan Hadfield-Menell, MIT
- Tan Zhi-Xuan, MIT
- Rui-Jie Yew, Brown University

Please send your inquiries to [rlsworganizers@gmail.com](mailto:rlsworganizers@gmail.com).
